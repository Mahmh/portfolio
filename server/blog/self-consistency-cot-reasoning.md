---
title: Chain-of-Thought + Self-Consistency = Better Answers
date: 2025-08-04
tags: [NLP, LLM, Prompting, Reasoning]
img: https://raw.githubusercontent.com/Mahmh/ml-research-papers/refs/heads/main/self-consistency-cot-reasoning/thumbnail.png
excerpt: Using Chain-of-Thought with Self-Consistency, sampling multiple reasoning paths and choosing the most common improves LLM accuracy.
---

## 1. Introduction
Chain-of-Thought (CoT) prompting helps large language models break down complex problems into intermediate steps. **Self-Consistency** builds on CoT by sampling multiple reasoning chains and aggregating their final answers. This simple ensemble trick often outperforms a single reasoning pass, making your LLM-powered applications more reliable.

## 2. Why Self-Consistency Helps
LLMs generate text autoregressively, and a single "best" decode might follow a suboptimal reasoning trajectory. By sampling $k$ different chains-of-thought and then selecting the most frequent final answer, Self-Consistency mitigates occasional reasoning missteps. In effect, it acts like an internal vote among candidate solutions, smoothing out noise in any one chain.

## 3. How It Works
1. **Prompt with CoT cues:** As before, include "Let's think step by step."  
2. **Enable sampling:** Use a temperature value higher than 0 or top-$k$ sampling to generate $N$ distinct CoT outputs.  
3. **Parse final answers:** Extract the answer token or string from each sampled chain.  
4. **Aggregate by majority vote:** Choose the answer that appears most often across samples.

```py
prompt = "Q: If there are 23 cookies and you eat 7, how many are left? Let's think step by step."
outputs = model.generate(prompt, num_samples=10, temperature=0.7)
answers = [extract_answer(o) for o in outputs]
final_answer = most_common(answers)
```

## 4. Example: Self-Consistency in Action
Ask the same cookie question with $N$ = 5 sampled chains:

```yml
Chain 1:
  1. Start with 23 cookies.
  2. Subtract 7.
  3. 23 - 7 = 16.
Answer: 16

Chain 2:
  1. You had 23.
  2. You eat 7.
  3. 23 minus 7 equals 16.
Answer: 16

Chain 3:
  1. Total cookies 23.
  2. Eaten 7 leaves 16.
Answer: 16

Chain 4:
  1. Begin 23.
  2. Eat 7.
  3. Result is 15.  # occasional glitch
Answer: 15

Chain 5:
  1. 23 cookies originally.
  2. Remove 7.
  3. 23 − 7 = 16.
Answer: 16
```

Here, four out of five chains yield **16**, so Self-Consistency picks 16 as the final answer, overriding the single erroneous 15.

## 5. When to Use Self-Consistency
* **High-stakes outputs:** Use it when an incorrect answer is costly (e.g., medical, legal Q\&A).
* **Logic puzzles & math:** Especially effective on problems that require multi-step deduction.
* **Ambiguous tasks:** When there are multiple plausible reasoning paths, majority voting helps stabilize the result.

## 6. Tips for Effective Self-Consistency
* **Balance sample size vs. cost:** Typical $N$ = 5–20; more samples improve robustness but increase compute.
* **Tune sampling parameters:** A moderate temperature (0.5–0.8) often yields diverse yet coherent chains.
* **Automate answer extraction:** Use regex or simple parsers to pull out the final line from each chain.
* **Cache intermediate chains:** For repeated questions, reuse past chains to save API calls.

## 7. Limitations & Caveats
* **Computational overhead:** Sampling multiple chains multiplies API usage and latency.
* **Aggregation bias:** If all samples share the same flawed premise, majority vote won't help.
* **Parsing errors:** Reliable answer extraction depends on consistent formatting of the model's output.

## 8. Conclusion
Self-Consistency is a powerful extension to Chain-of-Thought prompting. By ensembling multiple reasoning paths, it dramatically reduces one-off mistakes. Next time you build an LLM pipeline for reasoning tasks, give Self-Consistency a try and see how ensemble wisdom can boost your model's accuracy.

## 9. Try It Yourself
I wrote a small Jupyter notebook for demonstrating Self-Consistency vs. Greedy Decoding. You can find it in this [GitHub repository](https://github.com/Mahmh/ml-research-papers/tree/main/self-consistency-cot-reasoning).

Here, I experimented with different number of diverse outputs (sample count) generated by this model to evaluate it on 100 samples from the [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k/viewer/main/train?row=7294&views%5B%5D=main_train), which took me approximately 1 hour. You can see that, in general, a higher sample count increases the probability of the model's answers to be correct.

![Accuracy vs. Sample Count graph](https://raw.githubusercontent.com/Mahmh/ml-research-papers/refs/heads/main/self-consistency-cot-reasoning/graph.jpg)

---

Thank you for reading! If you found this explanation helpful, feel free to:
- Share with the NLP community.
- Star [this repository](https://github.com/Mahmh/ml-research-papers/tree/main).
- [Work with me](/contact) – I'm open to collaborations and freelance projects.